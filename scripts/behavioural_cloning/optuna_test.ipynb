{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Sampler, BatchSampler, Dataset\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, images, actions):\n",
    "        self._images = images\n",
    "        self._actions = actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self._images[index], self._actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "class RandomBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_length = len(dataset)\n",
    "        self.n_batches = self.dataset_length / self.batch_size\n",
    "        self.batch_ids = torch.randperm(int(self.n_batches))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for id in self.batch_ids:\n",
    "            idx = torch.arange(id * self.batch_size, (id + 1) * self.batch_size)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "        if int(self.n_batches) < self.n_batches:\n",
    "            idx = torch.arange(int(self.n_batches) * self.batch_size, self.dataset_length)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "\n",
    "\n",
    "def fast_loader(dataset, batch_size=32, drop_last=False, transforms=None):\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=None,\n",
    "        sampler=BatchSampler(RandomBatchSampler(dataset, batch_size), batch_size=batch_size, drop_last=drop_last)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = tables.open_file('tmnf_1os.hdf5', mode=\"r\")\n",
    "images = np.moveaxis(hdf5_file.root.images[:], 3, 1)\n",
    "actions = hdf5_file.root.actions[:]\n",
    "hdf5_file.close()\n",
    "\n",
    "size = images.shape[0]\n",
    "fold = 0.3\n",
    "rng = np.random.default_rng()\n",
    "indexes = rng.choice(size, size=int(fold * size), replace=False)\n",
    "\n",
    "images = images[indexes]\n",
    "actions = actions[indexes]\n",
    "\n",
    "images = torch.tensor(images / 255, dtype=torch.float32)\n",
    "actions = torch.tensor(actions, dtype=torch.float32)\n",
    "\n",
    "train_images, test_images, train_actions, test_actions = train_test_split(\n",
    "    images, actions, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExpertDataset(train_images, train_actions)\n",
    "test_dataset = ExpertDataset(test_images, test_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, conv_layers_types: list[str], conv_layers_outputs: list[int], kernel_sizes: list[int], strides: list[int]):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        inputs = 4\n",
    "        outputs = 4\n",
    "        convw, convh = 53, 150\n",
    "        layers = []\n",
    "\n",
    "        for layer_type, layer_outputs, kernel_size, stride in zip(conv_layers_types, conv_layers_outputs, kernel_sizes, strides):\n",
    "            layers.append(nn.Conv2d(inputs, layer_outputs, kernel_size=kernel_size, stride=stride, padding=0))\n",
    "            if layer_type == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif layer_type == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            inputs = layer_outputs\n",
    "            convw = (convw - kernel_size) // stride + 1\n",
    "            convh = (convh - kernel_size) // stride + 1\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(inputs * convw * convh, outputs))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layers_sequence = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers_sequence(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0., 0.\n",
    "    threshod = 0.5\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = pred > threshod\n",
    "            correct += (pred == y).type(torch.float).sum().item() == len(y[0])\n",
    "            # correct += (pred == y).type(torch.float).sum().item() / len(y[0])\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    # print(f\"test accuracy: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\")\n",
    "    return correct * 100, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    # layer_1 = trial.suggest_categorical(\"layer_1\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
    "    # layer_2 = trial.suggest_categorical(\"layer_2\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
    "\n",
    "    # units_1 = trial.suggest_int(\"units_1\", 300, 600, step=25)\n",
    "    # units_2 = trial.suggest_int(\"units_2\", 150, 350, step=25)\n",
    "\n",
    "    layer_1 = \"relu\"\n",
    "    layer_2 = \"tanh\"\n",
    "    layer_3 = \"tanh\"\n",
    "\n",
    "    outs_1 = 32\n",
    "    outs_2 = 64\n",
    "    outs_3 = 64\n",
    "\n",
    "    kernel_size_1 = 8\n",
    "    kernel_size_2 = 4\n",
    "    kernel_size_3 = 3\n",
    "    \n",
    "    stride_1 = 4\n",
    "    stride_2 = 2\n",
    "    stride_3 = 1\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 512, log=True)\n",
    "\n",
    "    layers = [layer_1, layer_2, layer_3]\n",
    "    layers_out = [outs_1, outs_2, outs_3]\n",
    "    kernel_sizes = [kernel_size_1, kernel_size_2, kernel_size_3]\n",
    "    strides = [stride_1, stride_2, stride_3]\n",
    "\n",
    "    model = NeuralNetwork(layers, layers_out, kernel_sizes, strides).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = fast_loader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = fast_loader(test_dataset, batch_size=1)\n",
    "\n",
    "\n",
    "    global best\n",
    "    for epoch in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        acc, loss = test(test_dataloader, model, loss_fn)\n",
    "        \n",
    "        if acc > best:\n",
    "            best = acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'acc': acc\n",
    "                }, f'{trial.number}-{epoch}-acc{int(acc)}')\n",
    "    return best\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:14:10,398]\u001b[0m A new study created in memory with name: no-name-a794e7a9-e12a-4a3f-968d-f071d4335693\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in memory with name: no-name-a794e7a9-e12a-4a3f-968d-f071d4335693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:19:15,970]\u001b[0m Trial 0 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.0012857756342430254, 'batch_size': 33}. Best is trial 0 with value: 34.40673767649244.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.0012857756342430254, 'batch_size': 33}. Best is trial 0 with value: 34.40673767649244.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:23:47,897]\u001b[0m Trial 1 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.006147973187478454, 'batch_size': 67}. Best is trial 0 with value: 34.40673767649244.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.006147973187478454, 'batch_size': 67}. Best is trial 0 with value: 34.40673767649244.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:28:24,229]\u001b[0m Trial 2 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.0035108553003790565, 'batch_size': 302}. Best is trial 0 with value: 34.40673767649244.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with value: 34.40673767649244 and parameters: {'learning_rate': 0.0035108553003790565, 'batch_size': 302}. Best is trial 0 with value: 34.40673767649244.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:32:55,694]\u001b[0m Trial 3 finished with value: 45.776566757493185 and parameters: {'learning_rate': 0.001609025053144712, 'batch_size': 166}. Best is trial 3 with value: 45.776566757493185.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished with value: 45.776566757493185 and parameters: {'learning_rate': 0.001609025053144712, 'batch_size': 166}. Best is trial 3 with value: 45.776566757493185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:37:16,145]\u001b[0m Trial 4 finished with value: 51.22615803814714 and parameters: {'learning_rate': 1.1657425177614926e-05, 'batch_size': 33}. Best is trial 4 with value: 51.22615803814714.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished with value: 51.22615803814714 and parameters: {'learning_rate': 1.1657425177614926e-05, 'batch_size': 33}. Best is trial 4 with value: 51.22615803814714.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:41:29,080]\u001b[0m Trial 5 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.0016062304850273166, 'batch_size': 411}. Best is trial 5 with value: 57.666584097101804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.0016062304850273166, 'batch_size': 411}. Best is trial 5 with value: 57.666584097101804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:45:39,628]\u001b[0m Trial 6 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.00015714200043781185, 'batch_size': 391}. Best is trial 5 with value: 57.666584097101804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.00015714200043781185, 'batch_size': 391}. Best is trial 5 with value: 57.666584097101804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:49:50,332]\u001b[0m Trial 7 finished with value: 57.666584097101804 and parameters: {'learning_rate': 2.193051350055647e-05, 'batch_size': 512}. Best is trial 5 with value: 57.666584097101804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 finished with value: 57.666584097101804 and parameters: {'learning_rate': 2.193051350055647e-05, 'batch_size': 512}. Best is trial 5 with value: 57.666584097101804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:54:01,792]\u001b[0m Trial 8 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.005737614525998634, 'batch_size': 232}. Best is trial 5 with value: 57.666584097101804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.005737614525998634, 'batch_size': 232}. Best is trial 5 with value: 57.666584097101804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-28 20:58:33,675]\u001b[0m Trial 9 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.004440522787669779, 'batch_size': 142}. Best is trial 5 with value: 57.666584097101804.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 finished with value: 57.666584097101804 and parameters: {'learning_rate': 0.004440522787669779, 'batch_size': 142}. Best is trial 5 with value: 57.666584097101804.\n",
      "Study statistics: \n",
      "  Number of finished trials:  10\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  10\n",
      "Best trial:\n",
      "  Value:  57.666584097101804\n",
      "  Params: \n",
      "    learning_rate: 0.0016062304850273166\n",
      "    batch_size: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py310\\lib\\site-packages\\optuna\\structs.py:18: FutureWarning: `structs` is deprecated. Classes have moved to the following modules. `structs.StudyDirection`->`study.StudyDirection`, `structs.StudySummary`->`study.StudySummary`, `structs.FrozenTrial`->`trial.FrozenTrial`, `structs.TrialState`->`trial.TrialState`, `structs.TrialPruned`->`exceptions.TrialPruned`.\n",
      "  warnings.warn(_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc10dd72c8aaf2d861c41511aea097bfd1dfda640e26e44598e27f4cfd3593d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
