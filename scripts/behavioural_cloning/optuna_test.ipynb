{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Sampler, BatchSampler, Dataset\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tables\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, images, actions):\n",
    "        self._images = images\n",
    "        self._actions = actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self._images[index], self._actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "class RandomBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_length = len(dataset)\n",
    "        self.n_batches = self.dataset_length / self.batch_size\n",
    "        self.batch_ids = torch.randperm(int(self.n_batches))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for id in self.batch_ids:\n",
    "            idx = torch.arange(id * self.batch_size, (id + 1) * self.batch_size)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "        if int(self.n_batches) < self.n_batches:\n",
    "            idx = torch.arange(int(self.n_batches) * self.batch_size, self.dataset_length)\n",
    "            for index in idx:\n",
    "                yield int(index)\n",
    "\n",
    "\n",
    "def fast_loader(dataset, batch_size=32, drop_last=False, transforms=None):\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=None,\n",
    "        sampler=BatchSampler(RandomBatchSampler(dataset, batch_size), batch_size=batch_size, drop_last=drop_last)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = tables.open_file('tmnf_1os.hdf5', mode=\"r\")\n",
    "images = np.moveaxis(hdf5_file.root.images[:], 3, 1)\n",
    "actions = hdf5_file.root.actions[:]\n",
    "hdf5_file.close()\n",
    "\n",
    "size = images.shape[0]\n",
    "fold = 0.1\n",
    "rng = np.random.default_rng()\n",
    "indexes = rng.choice(size, size=int(fold * size), replace=False)\n",
    "\n",
    "images = images[indexes]\n",
    "actions = actions[indexes]\n",
    "\n",
    "images = torch.tensor(images / 255, dtype=torch.float32)\n",
    "actions = torch.tensor(actions, dtype=torch.float32)\n",
    "\n",
    "train_images, test_images, train_actions, test_actions = train_test_split(\n",
    "    images, actions, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExpertDataset(train_images, train_actions)\n",
    "test_dataset = ExpertDataset(test_images, test_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, conv_layers_types: list[str], conv_layers_outputs: list[int], kernel_sizes: list[int], strides: list[int]):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        inputs = 4\n",
    "        outputs = 4\n",
    "        convw, convh = 53, 150\n",
    "        layers = []\n",
    "\n",
    "        for layer_type, layer_outputs, kernel_size, stride in zip(conv_layers_types, conv_layers_outputs, kernel_sizes, strides):\n",
    "            layers.append(nn.Conv2d(inputs, layer_outputs, kernel_size=kernel_size, stride=stride, padding=0))\n",
    "            if layer_type == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif layer_type == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            inputs = layer_outputs\n",
    "            convw = (convw - kernel_size) // stride + 1\n",
    "            convh = (convh - kernel_size) // stride + 1\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(inputs * convw * convh, outputs))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layers_sequence = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers_sequence(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0., 0.\n",
    "    threshod = 0.5\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = pred > threshod\n",
    "            correct += (pred == y).type(torch.float).sum().item() / len(y[0])\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    # print(f\"test accuracy: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\")\n",
    "    return correct * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    # layer_1 = trial.suggest_categorical(\"layer_1\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
    "    # layer_2 = trial.suggest_categorical(\"layer_2\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
    "\n",
    "    # units_1 = trial.suggest_int(\"units_1\", 300, 600, step=25)\n",
    "    # units_2 = trial.suggest_int(\"units_2\", 150, 350, step=25)\n",
    "\n",
    "    layer_1 = \"relu\"\n",
    "    layer_2 = \"tanh\"\n",
    "    layer_3 = \"tanh\"\n",
    "\n",
    "    outs_1 = 32\n",
    "    outs_2 = 64\n",
    "    outs_3 = 64\n",
    "\n",
    "    kernel_size_1 = 8\n",
    "    kernel_size_2 = 4\n",
    "    kernel_size_3 = 3\n",
    "    \n",
    "    stride_1 = 4\n",
    "    stride_2 = 2\n",
    "    stride_3 = 1\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 512, log=True)\n",
    "\n",
    "    layers = [layer_1, layer_2, layer_3]\n",
    "    layers_out = [outs_1, outs_2, outs_3]\n",
    "    kernel_sizes = [kernel_size_1, kernel_size_2, kernel_size_3]\n",
    "    strides = [stride_1, stride_2, stride_3]\n",
    "\n",
    "    model = NeuralNetwork(layers, layers_out, kernel_sizes, strides).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = fast_loader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = fast_loader(test_dataset, batch_size=1)\n",
    "\n",
    "    best = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        score = test(test_dataloader, model, loss_fn)\n",
    "        \n",
    "        if score > best:\n",
    "            best = score\n",
    "            global best_model\n",
    "            best_model = model\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-23 00:36:00,951]\u001b[0m A new study created in memory with name: no-name-fc32d48b-ad33-467c-93c5-2811cead236e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in memory with name: no-name-fc32d48b-ad33-467c-93c5-2811cead236e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-23 00:37:34,149]\u001b[0m Trial 0 finished with value: 78.54754829123328 and parameters: {'learning_rate': 1.745149104583636e-05, 'batch_size': 287}. Best is trial 0 with value: 78.54754829123328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with value: 78.54754829123328 and parameters: {'learning_rate': 1.745149104583636e-05, 'batch_size': 287}. Best is trial 0 with value: 78.54754829123328.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-23 00:39:07,863]\u001b[0m Trial 1 finished with value: 80.12630014858841 and parameters: {'learning_rate': 7.772904340337982e-05, 'batch_size': 325}. Best is trial 1 with value: 80.12630014858841.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with value: 80.12630014858841 and parameters: {'learning_rate': 7.772904340337982e-05, 'batch_size': 325}. Best is trial 1 with value: 80.12630014858841.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-23 00:40:44,072]\u001b[0m Trial 2 finished with value: 78.8261515601783 and parameters: {'learning_rate': 3.4967260525987386e-05, 'batch_size': 216}. Best is trial 1 with value: 80.12630014858841.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with value: 78.8261515601783 and parameters: {'learning_rate': 3.4967260525987386e-05, 'batch_size': 216}. Best is trial 1 with value: 80.12630014858841.\n",
      "Study statistics: \n",
      "  Number of finished trials:  3\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  3\n",
      "Best trial:\n",
      "  Value:  80.12630014858841\n",
      "  Params: \n",
      "    learning_rate: 7.772904340337982e-05\n",
      "    batch_size: 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py310\\lib\\site-packages\\optuna\\structs.py:18: FutureWarning: `structs` is deprecated. Classes have moved to the following modules. `structs.StudyDirection`->`study.StudyDirection`, `structs.StudySummary`->`study.StudySummary`, `structs.FrozenTrial`->`trial.FrozenTrial`, `structs.TrialState`->`trial.TrialState`, `structs.TrialPruned`->`exceptions.TrialPruned`.\n",
      "  warnings.warn(_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc10dd72c8aaf2d861c41511aea097bfd1dfda640e26e44598e27f4cfd3593d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
